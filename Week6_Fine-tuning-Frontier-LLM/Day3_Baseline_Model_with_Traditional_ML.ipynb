{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mXSMakjNMa0"
      },
      "source": [
        "# The Product Pricer Continued\n",
        "\n",
        "A model that can estimate how much something costs, from its description.\n",
        "\n",
        "## Baseline Models\n",
        "\n",
        "Today we work on the simplest models to act as a starting point that we will beat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wUMvaLqNNBY"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "from google.colab import userdata\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2dOBbl0Nd_D"
      },
      "outputs": [],
      "source": [
        "# More imports for our traditional machine learning\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KL3ULMHNpG0"
      },
      "source": [
        "## NLP imports\n",
        "\n",
        "In the next cell, we have more imports for our NLP related machine learning.  \n",
        "If the gensim import gives you an error like \"Cannot import name 'triu' from 'scipy.linalg' then please run in another cell:  \n",
        "`!pip install \"scipy<1.13\"`  \n",
        "As described on StackOverflow [here](https://stackoverflow.com/questions/78279136/importerror-cannot-import-name-triu-from-scipy-linalg-when-importing-gens).  \n",
        "Many thanks to students Arnaldo G and Ard V for sorting this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ldR6wfmOHGw",
        "outputId": "8c4b34f9-ec0b-4000-853c-2b94fd9850d1"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "5ArueilNN0F9",
        "outputId": "81b39cce-5bd5-4324-f1eb-92f0fff023c9"
      },
      "outputs": [],
      "source": [
        "# NLP related imports\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChEU9QDWOQ1x"
      },
      "outputs": [],
      "source": [
        "# Finally, more imports for more advanced machine learning\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8ph8plSNXhy"
      },
      "outputs": [],
      "source": [
        "# environment and login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "# login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcMftBJZNb-o"
      },
      "outputs": [],
      "source": [
        "# Constants - used for printing to stdout in color\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQl5ixlNOrbW"
      },
      "outputs": [],
      "source": [
        "# Loading the pickle files:\n",
        "\n",
        "with open('train_lite.pkl', 'rb') as file:\n",
        "    train = pickle.load(file)\n",
        "\n",
        "with open('test_lite.pkl', 'rb') as file:\n",
        "    test = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ATKkyCRPGOR"
      },
      "outputs": [],
      "source": [
        "print(train[5].prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o7B5blaPkd4"
      },
      "outputs": [],
      "source": [
        "print(test[5].test_prompt())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV4txWS-QPdW"
      },
      "source": [
        "## Unveiling a mighty script that we will use a lot!\n",
        "\n",
        "A rather pleasing Test Harness that will evaluate any model against 250 items from the Test set\n",
        "\n",
        "And show us the results in a visually satisfying way.\n",
        "\n",
        "You write a function of this form:\n",
        "\n",
        "```\n",
        "def my_prediction_function(item):\n",
        "    # my code here\n",
        "    return my_estimate\n",
        "```\n",
        "\n",
        "And then you call:\n",
        "\n",
        "`Tester.test(my_prediction_function)`\n",
        "\n",
        "To evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2SW1HvLQy92"
      },
      "outputs": [],
      "source": [
        "class Tester:\n",
        "\n",
        "    def __init__(self, predictor, title=None, data=test, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size\n",
        "        self.guesses = []\n",
        "        self.truths = []\n",
        "        self.errors = []\n",
        "        self.sles = []\n",
        "        self.colors = []\n",
        "\n",
        "    def color_for(self, error, truth):\n",
        "        if error<40 or error/truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error<80 or error/truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint)\n",
        "        truth = datapoint.price\n",
        "        error = abs(guess - truth)\n",
        "        log_error = math.log(truth+1) - math.log(guess+1)\n",
        "        sle = log_error ** 2\n",
        "        color = self.color_for(error, truth)\n",
        "        title = datapoint.title if len(datapoint.title) <= 40 else datapoint.title[:40]+\"...\"\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color==\"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    @classmethod\n",
        "    def test(cls, function):\n",
        "        cls(function).run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85wYp3_QU3Qn"
      },
      "source": [
        "# Now for something basic\n",
        "\n",
        "What's the very simplest model you could imagine?\n",
        "\n",
        "Let's start with a random number generator!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bphzyk1cU3yu"
      },
      "outputs": [],
      "source": [
        "def random_pricer(item):\n",
        "    return random.randrange(1,200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWCNLp6aU9HO"
      },
      "outputs": [],
      "source": [
        "# Set the random seed\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Run our TestRunner\n",
        "Tester.test(random_pricer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjs50kCsU_Nx"
      },
      "outputs": [],
      "source": [
        "# That was fun!\n",
        "# We can do better - here's another rather trivial model\n",
        "\n",
        "training_prices = [item.price for item in train]\n",
        "training_average = sum(training_prices) / len(training_prices)\n",
        "\n",
        "def constant_pricer(item):\n",
        "    return training_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXxKEekyWd2p"
      },
      "outputs": [],
      "source": [
        "# Run our constant predictor\n",
        "Tester.test(constant_pricer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-VhvN3rl_bz"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ANne9kLmBeC"
      },
      "outputs": [],
      "source": [
        "train[0].details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB-6RHm9mGsf"
      },
      "outputs": [],
      "source": [
        "# Create a new \"features\" field on items, and populate it with json parsed from the details dict\n",
        "\n",
        "for item in train:\n",
        "    item.features = json.loads(item.details)\n",
        "for item in test:\n",
        "    item.features = json.loads(item.details)\n",
        "\n",
        "# Look at one\n",
        "\n",
        "train[0].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8fWD5Z6mXSC"
      },
      "outputs": [],
      "source": [
        "# Look at 20 most common features in training set\n",
        "\n",
        "feature_count = Counter()\n",
        "for item in train:\n",
        "    for f in item.features.keys():\n",
        "        feature_count[f]+=1\n",
        "\n",
        "feature_count.most_common(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGbi299nm6Th"
      },
      "outputs": [],
      "source": [
        "# Now some janky code to pluck out the Item Weight\n",
        "# Don't worry too much about this: spoiler alert, it's not going to be much use in training!\n",
        "\n",
        "def get_weight(item):\n",
        "    weight_str = item.features.get('Item Weight')\n",
        "    if weight_str:\n",
        "        parts = weight_str.split(' ')\n",
        "        amount = float(parts[0])\n",
        "        unit = parts[1].lower()\n",
        "        if unit==\"pounds\":\n",
        "            return amount\n",
        "        elif unit==\"ounces\":\n",
        "            return amount / 16\n",
        "        elif unit==\"grams\":\n",
        "            return amount / 453.592\n",
        "        elif unit==\"milligrams\":\n",
        "            return amount / 453592\n",
        "        elif unit==\"kilograms\":\n",
        "            return amount / 0.453592\n",
        "        elif unit==\"hundredths\" and parts[2].lower()==\"pounds\":\n",
        "            return amount / 100\n",
        "        else:\n",
        "            print(weight_str)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfnqkbLRoGzl"
      },
      "outputs": [],
      "source": [
        "weights = [get_weight(t) for t in train]\n",
        "weights = [w for w in weights if w]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htS4mwjcoYfL"
      },
      "outputs": [],
      "source": [
        "average_weight = sum(weights)/len(weights)\n",
        "average_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqrtrQi1piwF"
      },
      "outputs": [],
      "source": [
        "def get_weight_with_default(item):\n",
        "    weight = get_weight(item)\n",
        "    return weight or average_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xduHD9fHp2Jf"
      },
      "outputs": [],
      "source": [
        "def get_rank(item):\n",
        "    rank_dict = item.features.get(\"Best Sellers Rank\")\n",
        "    if rank_dict:\n",
        "        ranks = rank_dict.values()\n",
        "        return sum(ranks)/len(ranks)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1c6aiOpqUoc"
      },
      "outputs": [],
      "source": [
        "ranks = [get_rank(t) for t in train]\n",
        "ranks = [r for r in ranks if r]\n",
        "average_rank = sum(ranks)/len(ranks)\n",
        "average_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r-80hQFqnb8"
      },
      "outputs": [],
      "source": [
        "def get_rank_with_default(item):\n",
        "    rank = get_rank(item)\n",
        "    return rank or average_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjSjS-1mquZi"
      },
      "outputs": [],
      "source": [
        "def get_text_length(item):\n",
        "    return len(item.test_prompt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXdXxV2Wqzyl"
      },
      "outputs": [],
      "source": [
        "# investigate the brands\n",
        "\n",
        "brands = Counter()\n",
        "for t in train:\n",
        "    brand = t.features.get(\"Manufacturer\")\n",
        "    if brand:\n",
        "        brands[brand]+=1\n",
        "\n",
        "# Look at most common 40 brands\n",
        "\n",
        "brands.most_common(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvBa3Y3kq6fT"
      },
      "outputs": [],
      "source": [
        "TOP_ELECTRONICS_BRANDS = [\n",
        "    \"ge\",\n",
        "    \"whirlpool\",\n",
        "    \"frigidaire\",\n",
        "    \"samsung\",\n",
        "    \"lg\",\n",
        "    \"electrolux\",\n",
        "    \"broan-nutone\",\n",
        "    \"bunn\",\n",
        "    \"costway\",\n",
        "    \"melitta\"\n",
        "]\n",
        "def is_top_electronics_brand(item):\n",
        "    brand = item.features.get(\"Manufacturer\")\n",
        "    return brand and brand.lower() in TOP_ELECTRONICS_BRANDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEP_ZzLJ0zdg"
      },
      "outputs": [],
      "source": [
        "# Adding my own two features of batteries required and batteries included\n",
        "\n",
        "def battery_required(item):\n",
        "  yes_no = item.features.get(\"Batteries Required?\")\n",
        "  return 1 if yes_no == \"Yes\" else 0\n",
        "\n",
        "def battery_included(item):\n",
        "  yes_no = item.features.get(\"Batteries Included?\")\n",
        "  return 1 if yes_no == \"Yes\" else 0\n",
        "\n",
        "battery_require = [item for item in train if item.features.get(\"Batteries Required?\")\n",
        "and battery_required(item) == 1]\n",
        "print(len(battery_require))\n",
        "\n",
        "battery_include = [item for item in train if item.features.get(\"Batteries Included?\")\n",
        "and battery_included(item) == 1]\n",
        "print(len(battery_include))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYg5GBWSrsq9"
      },
      "outputs": [],
      "source": [
        "def get_features(item):\n",
        "    return {\n",
        "        \"weight\": get_weight_with_default(item),\n",
        "        # \"rank\": get_rank_with_default(item),\n",
        "        # \"text_length\": get_text_length(item),\n",
        "        # \"is_top_electronics_brand\": 1 if is_top_electronics_brand(item) else 0,\n",
        "        # \"batteries_required?\": battery_required(item),\n",
        "        # \"batteries_included?\": battery_included(item)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-kQSV4fyCEu"
      },
      "outputs": [],
      "source": [
        "top_brands = [item for item in train if item.features.get('Manufacturer') and\n",
        "item.features.get('Manufacturer').lower() in TOP_ELECTRONICS_BRANDS]\n",
        "len(top_brands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOkTbhAgrwMl"
      },
      "outputs": [],
      "source": [
        "get_features(train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDpkoqU-sKK0"
      },
      "source": [
        "# Into ML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8fzKSVysL12"
      },
      "outputs": [],
      "source": [
        "# A utility function to convert our features into a pandas dataframe\n",
        "\n",
        "def list_to_dataframe(items):\n",
        "    features = [get_features(item) for item in items]\n",
        "    df = pd.DataFrame(features)\n",
        "    df['price'] = [item.price for item in items]\n",
        "    return df\n",
        "\n",
        "train_df = list_to_dataframe(train)\n",
        "test_df = list_to_dataframe(test[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2squX3SysN58"
      },
      "outputs": [],
      "source": [
        "# Traditional Linear Regression!\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Separate features and target\n",
        "feature_columns = ['weight']\n",
        "\n",
        "X_train = train_df[feature_columns]\n",
        "y_train = train_df['price']\n",
        "X_test = test_df[feature_columns]\n",
        "y_test = test_df['price']\n",
        "\n",
        "# Train a Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "for feature, coef in zip(feature_columns, model.coef_):\n",
        "    print(f\"{feature}: {coef}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "\n",
        "# Predict the test set and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRDCUWoPtVGn"
      },
      "outputs": [],
      "source": [
        "# Function to predict price for a new item\n",
        "\n",
        "def linear_regression_pricer(item):\n",
        "    features = get_features(item)\n",
        "    features_df = pd.DataFrame([features])\n",
        "    return model.predict(features_df)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RULkgajcuJ8n"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "\n",
        "Tester.test(linear_regression_pricer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0hVt2pI4fox"
      },
      "source": [
        "# Into NLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_hDVCmS4hKK"
      },
      "outputs": [],
      "source": [
        "# For the next few models, we prepare our documents and prices\n",
        "# Note that we use the test prompt for the documents, otherwise we'll reveal the answer!!\n",
        "\n",
        "prices = np.array([float(item.price) for item in train])\n",
        "documents = [item.test_prompt() for item in train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S10dUPnL481S"
      },
      "outputs": [],
      "source": [
        "# Use the CountVectorizer for a Bag of Words model\n",
        "\n",
        "np.random.seed(42)\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH-1trYL5bee"
      },
      "outputs": [],
      "source": [
        "def bow_lr_pricer(item):\n",
        "    x = vectorizer.transform([item.test_prompt()])\n",
        "    return max(regressor.predict(x)[0], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny9IiNg75k4O"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "\n",
        "Tester.test(bow_lr_pricer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGHjqoRp6QFd"
      },
      "source": [
        "# The amazing word2vec model, implemented in gensim NLP library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJmHk5ZK6QhP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Preprocess the documents\n",
        "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=processed_docs, vector_size=400, window=5, min_count=1, workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHCz9ozI7Ssn"
      },
      "outputs": [],
      "source": [
        "# This step of averaging vectors across the document is a weakness in our approach\n",
        "\n",
        "def document_vector(doc):\n",
        "    doc_words = simple_preprocess(doc)\n",
        "    word_vectors = [w2v_model.wv[word] for word in doc_words if word in w2v_model.wv]\n",
        "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(w2v_model.vector_size)\n",
        "\n",
        "# Create feature matrix\n",
        "X_w2v = np.array([document_vector(doc) for doc in documents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtLoZlWT7lr3"
      },
      "outputs": [],
      "source": [
        "# Run Linear Regression on word2vec\n",
        "\n",
        "word2vec_lr_regressor = LinearRegression()\n",
        "word2vec_lr_regressor.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92yv0m4q7sbR"
      },
      "outputs": [],
      "source": [
        "def word2vec_lr_pricer(item):\n",
        "    doc = item.test_prompt()\n",
        "    doc_vector = document_vector(doc)\n",
        "    return max(0, word2vec_lr_regressor.predict([doc_vector])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7GtTk_A7un4"
      },
      "outputs": [],
      "source": [
        "Tester.test(word2vec_lr_pricer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXKtGHpB8EsD"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POnm0lNh8FHC"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "svr_regressor = LinearSVR()\n",
        "\n",
        "svr_regressor.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWCRHMxr8JrO"
      },
      "outputs": [],
      "source": [
        "def svr_pricer(item):\n",
        "    np.random.seed(42)\n",
        "    doc = item.test_prompt()\n",
        "    doc_vector = document_vector(doc)\n",
        "    return max(float(svr_regressor.predict([doc_vector])[0]),0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvauyAtf8MWz"
      },
      "outputs": [],
      "source": [
        "Tester.test(svr_pricer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVCSAbCS8fBt"
      },
      "source": [
        "# And the powerful Random Forest regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILmvjHL18fYB"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=8)\n",
        "rf_model.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4HwCB-f8hNK"
      },
      "outputs": [],
      "source": [
        "def random_forest_pricer(item):\n",
        "    doc = item.test_prompt()\n",
        "    doc_vector = document_vector(doc)\n",
        "    return max(0, rf_model.predict([doc_vector])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m3HVeDk9AyK"
      },
      "outputs": [],
      "source": [
        "Tester.test(random_forest_pricer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
